{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBdb33NDBDd4"
      },
      "source": [
        "# Parameter tuning\n",
        "Performs nested cross validation on the training set to select the best parameters for each model\n",
        "\n",
        "Training set:\n",
        "\n",
        "1.   MIMIC\n",
        "2.   HEPAR\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## MIMIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4OTrznzBP9Y",
        "outputId": "d43bd874-2c4b-464c-c931-11efc7d0d3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "from google.colab import drive\n",
        "ROOT_PATH = '/content/drive'\n",
        "drive.mount(ROOT_PATH)\n",
        "ROOT_PATH += '/My Drive/XAI'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpHeW7CcCGJS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD-IdJKFE0HW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Defining pipeline and classifiers\n",
        "clf_dict = {'rf':RandomForestClassifier(random_state=1, n_jobs=-1),\n",
        "            'nn':MLPClassifier(random_state=1,hidden_layer_sizes=(100, 50, 20), max_iter=1000),\n",
        "            'lr':LogisticRegression(random_state=1, max_iter=500), 'gb':GradientBoostingClassifier(random_state=1), \n",
        "            }\n",
        "\n",
        "\n",
        "clf_param = {'lr': {\"penalty\":[\"l2\", \"l1\"], \"C\":[0.001, 0.5, 1], \"solver\":[\"liblinear\"]},\n",
        "             'rf':{\"n_estimators\":[100,500],\n",
        "                   \"max_features\": [\"auto\", \"sqrt\"],\n",
        "                   \"max_depth\": [10, 50, 100]},\n",
        "             'gb':{\"n_estimators\": [ 100, 500],\n",
        "                 \"min_samples_leaf\": [1, 4],\n",
        "                 \"max_features\": [\"auto\", \"sqrt\"]},\n",
        "             'nn':{\"activation\":[\"logistic\", \"relu\"],\n",
        "                   \"solver\":[\"adam\", \"sgd\"],\n",
        "                   \"learning_rate\":[\"constant\", \"adaptive\"]}}\n",
        "\n",
        "\n",
        "\n",
        "# cross val function\n",
        "def run_nested_cv(X, y, clf_dict, clf_param, n_splits=5, scoring='f1'):\n",
        "  r = {}\n",
        "  cv_inner = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
        "  cv_outer = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
        "  for cl_name, clf in clf_dict.items():\n",
        "    print(cl_name)\n",
        "    if cl_name not in clf_param.keys():\n",
        "      print(cl_name + ' has no param defined')\n",
        "      continue\n",
        "    pipe = Pipeline([('scaler', MinMaxScaler()), ('classifier',clf)])\n",
        "    pgrid_new = {}\n",
        "    for k,v in clf_param[cl_name].items():\n",
        "      pgrid_new['classifier__'+k]=v\n",
        "    print(pgrid_new)\n",
        "    clf_search = GridSearchCV(pipe, param_grid=pgrid_new, scoring=scoring, cv=cv_inner)\n",
        "    nested_score = cross_val_score(clf_search, X=X, y=y, cv=cv_outer)\n",
        "    clf_search.fit(X, y)\n",
        "    r[cl_name] = clf_search\n",
        "  return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjzUfdFaCNoM",
        "outputId": "e7ba1d25-412a-4ef1-8a04-74ffa1f718ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame shape before NaN removal: (6000, 121)\n",
            "DataFrame shape after NaN removal: (5248, 48)\n",
            "Relative percentage of removed rows (wrt the old row size): 12.53\n",
            "Is there any NaN after NaN removal? False\n"
          ]
        }
      ],
      "source": [
        "#MIMIC PREPROCESSING\n",
        "df_raw = pd.read_csv(ROOT_PATH + '/Full_Dataset.csv')\n",
        "\n",
        "# Removal of features containing more than 90% of NaN\n",
        "original_shape = df_raw.shape\n",
        "print(\"DataFrame shape before NaN removal:\", original_shape)\n",
        "df = df_raw.dropna(thresh=round(df_raw.shape[0] * 0.90), axis=1)\n",
        "\n",
        "# NaN removal by removing rows with at least one NaN value\n",
        "# Check for NaN values\n",
        "if df.isnull().any().any():  # If there's at least one column with NaN, the output is \"True\"\n",
        "    old_row_size = original_shape[0]\n",
        "    df = df.dropna(axis=0, how='any')\n",
        "    print(\"DataFrame shape after NaN removal:\", df.shape)\n",
        "    new_row_size = df.shape[0]\n",
        "    print(\"Relative percentage of removed rows (wrt the old row size): %.2f\" % (\n",
        "            (old_row_size - new_row_size) / old_row_size * 100))\n",
        "\n",
        "# NaN values have now been removed, so the output should be \"False\"\n",
        "print(\"Is there any NaN after NaN removal?\", df.isnull().any().any())\n",
        "df = df.drop(columns=[\"recordid\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enS05wrQCitG",
        "outputId": "3dc97d7e-ed51-49f0-f374-8330c5687c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Gender', 'CCU', 'CSRU', 'SICU'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "#DATA PREPARATION\n",
        "pred_col = 'In-hospital_death' #edit this line when using another dataset\n",
        "\n",
        "X = df\n",
        "y = X[pred_col]\n",
        "X_feat = X.drop(columns=[pred_col])\n",
        "feature_names = X_feat.columns\n",
        "\n",
        "#check which features has less than 10 unique values to identify a subset of probable categorical features\n",
        "print(feature_names[np.where([len(np.unique(X_feat.iloc[:,i]))<10 for i in range(X_feat.shape[1])])])\n",
        "categorical_feat = ['CCU', 'CSU', 'SICU'] # edit this line when using another dataset\n",
        "iscat = [x in categorical_feat for x in feature_names]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_feat, y, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_c7d5BOKQOx",
        "outputId": "ab69ec4e-04ff-4f2e-d32e-8e3a6777baba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rf\n",
            "{'classifier__n_estimators': [100, 500], 'classifier__max_features': ['auto', 'sqrt'], 'classifier__max_depth': [10, 50, 100]}\n",
            "nn\n",
            "{'classifier__activation': ['logistic', 'relu'], 'classifier__solver': ['adam', 'sgd'], 'classifier__learning_rate': ['constant', 'adaptive']}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr\n",
            "{'classifier__penalty': ['l2', 'l1'], 'classifier__C': [0.001, 0.5, 1], 'classifier__solver': ['liblinear']}\n",
            "gb\n",
            "{'classifier__n_estimators': [100, 500], 'classifier__min_samples_leaf': [1, 4], 'classifier__max_features': ['auto', 'sqrt']}\n"
          ]
        }
      ],
      "source": [
        "r_mimic_ = run_nested_cv(X_train, y_train, clf_dict, clf_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qccGrDyBvupq"
      },
      "outputs": [],
      "source": [
        "def save_best_param(fpath, r):\n",
        "    with open(fpath, 'w') as o:\n",
        "        o.write('##### BEST PARAM AFTER 5-FOLD NESTED CV\\n')\n",
        "\n",
        "        for clname, vparam in r.items():\n",
        "            s = []\n",
        "            for k,v in vparam.best_params_.items():\n",
        "                s.append(k.split('__')[1] + '=' + str(v))\n",
        "            o.write(clname.upper()+':'+'|'.join(s)+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPVnn7zZFrLM"
      },
      "outputs": [],
      "source": [
        "save_best_param(ROOT_PATH + '/mimic_best_param.txt', r_mimic_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUCQeurRcJZb"
      },
      "outputs": [],
      "source": [
        "df_raw_iid = pd.read_csv(ROOT_PATH + '/HEPAR_simulated_patients.csv')\n",
        "df_raw_iid.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
        "\n",
        "def preproc_wrapper(df):\n",
        "  # Categorical variables handled\n",
        "  df_num = df.copy()\n",
        "  \"\"\"Handling categorical and nominal data\"\"\"\n",
        "\n",
        "  nominal_feat = ['age', 'triglycerides', 'bilirubin', 'phosphatase', 'proteins', 'platelet', \n",
        "                  'inr', 'urea', 'ESR', 'alt', 'ast','amylase', 'ggtp', 'cholesterol', 'albumin']\n",
        "\n",
        "  # preprocessing: data transformation\n",
        "  nominal_dict = {}\n",
        "  for n in nominal_feat:\n",
        "    unique_val = df[n].unique().tolist()\n",
        "    unique_num = [int(x.split('_')[1]) for x in unique_val]\n",
        "    val2num = dict(zip(unique_val, unique_num))\n",
        "    num2cat = dict(zip(sorted(unique_num), range(1, len(unique_num)+1)))\n",
        "    dict_n = {}\n",
        "    for k,v in val2num.items():\n",
        "      dict_n[k] = num2cat[v]\n",
        "    nominal_dict[n] = dict_n\n",
        "  # print(nominal_dict) ***DEBUG to check that it is consistent between the two datasets (it is, but it might not if using different datasets --> do not copy paste)\n",
        "\n",
        "  special_feat = ['ChHepatitis', 'sex', 'Cirrhosis']\n",
        "\n",
        "  dict_chhepa = {'absent':0, 'active':1, 'persistent':2}\n",
        "  dict_sex = {'female':1, 'male':2}\n",
        "  dict_cirr = {'absent':0, 'decompensate':1, 'compensate':2}\n",
        "\n",
        "  df_num['sex'] = [dict_sex[x] for x in df['sex']]\n",
        "  df_num['ChHepatitis'] = [dict_chhepa[x] for x in df['ChHepatitis']]\n",
        "  df_num['Cirrhosis'] = [dict_cirr[x] for x in df['Cirrhosis']]\n",
        "\n",
        "  categorical_feat = [x for x in df.columns if x not in nominal_feat+special_feat]\n",
        "  dict_cat = {'absent':0, 'present':1}\n",
        "\n",
        "  #print(categorical_feat)\n",
        "  for c in categorical_feat:\n",
        "    #print(c)\n",
        "    df_num[c] = [dict_cat[x] for x in df[c]]\n",
        "\n",
        "  for n in nominal_feat:\n",
        "    newcol = [nominal_dict[n][x] for x in df[n]]\n",
        "    df_num[n] = newcol\n",
        "\n",
        "  pred_col = 'hospital'\n",
        "\n",
        "  df_num[pred_col].value_counts()\n",
        "\n",
        "  np.random.seed(1)\n",
        "\n",
        "  dataset_class_1 = df_num[df_num[pred_col]==1]\n",
        "  dataset_class_0 = df_num[df_num[pred_col]==0]\n",
        "\n",
        "  # selecting a subpopulation of X% that will be used for training and testing ***DEBUG\n",
        "  # n_subpop = int(0.5*df_num.shape[0])\n",
        "  # i_class_yes = np.random.randint(0, high=dataset_class_1.shape[0], size=int(n_subpop/2))\n",
        "  # i_class_no = np.random.randint(0, high=dataset_class_0.shape[0], size=int(n_subpop/2))\n",
        "  # X = dataset_class_1.iloc[i_class_yes].append(dataset_class_0.iloc[i_class_no])\n",
        "\n",
        "  X = df_num\n",
        "  y = X[pred_col]\n",
        "  X_feat = X.drop(columns=[pred_col])\n",
        "  feature_names = X_feat.columns\n",
        "  iscat = [x in categorical_feat for x in feature_names]\n",
        "\n",
        "  return X_feat, y, feature_names, iscat\n",
        "\n",
        "X_iid, y_iid, feat_names_iid, iscat_iid = preproc_wrapper(df_raw_iid)\n",
        "\n",
        "feature_names = feat_names_iid # = feat_names_ood\n",
        "iscat = iscat_iid\n",
        "\n",
        "X_train_hepar, X_test_hepar, y_train_hepar, y_test_hepar = train_test_split(X_iid, y_iid, test_size=0.3, random_state=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6WuYQJpcvpe"
      },
      "outputs": [],
      "source": [
        "r_hepar = run_nested_cv(X_train_hepar, y_train_hepar, clf_dict, clf_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyOFd7IKdDho"
      },
      "outputs": [],
      "source": [
        "save_best_param(ROOT_PATH + '/hepar_best_param.txt', r_hepar)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}